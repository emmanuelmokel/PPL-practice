{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGepPpJ11kkfFWq9kOZLJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelmokel/PPL-practice/blob/main/NestedRHat_TFP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#tensorflow.__version__\n",
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk-kAi6cM7lc",
        "outputId": "38385ffb-5636-42fb-99c1-d9689b096b2d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.12.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U \"numpy~=1.23\"\n",
        "!rm -Rf probability\n",
        "!rm -Rf fun_mc\n",
        "!rm -Rf inference_gym\n",
        "!git clone https://github.com/tensorflow/probability.git\n",
        "!mv probability/spinoffs/fun_mc/fun_mc .\n",
        "!mv probability/spinoffs/inference_gym/inference_gym .\n",
        "#!pip install tf-nightly tfp-nightly jax jaxlib\n",
        "!pip install tensorflow==2.13.0\n",
        "!pip install tfp-nightly jax jaxlib\n",
        "\n",
        "!pip install immutabledict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yVfP9g3Tdncu",
        "outputId": "cf6393f2-104d-49d1-9270-515c64219cc5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'probability'...\n",
            "remote: Enumerating objects: 100117, done.\u001b[K\n",
            "remote: Counting objects: 100% (4943/4943), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 100117 (delta 4803), reused 4814 (delta 4763), pack-reused 95174\n",
            "Receiving objects: 100% (100117/100117), 137.02 MiB | 30.65 MiB/s, done.\n",
            "Resolving deltas: 100% (81925/81925), done.\n",
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.8.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.0)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, numpy, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.0\n",
            "    Uninstalling numpy-1.25.0:\n",
            "      Successfully uninstalled numpy-1.25.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tfp-nightly in /usr/local/lib/python3.10/dist-packages (0.20.0.dev20230707)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.10)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.10+cuda11.cudnn86)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (1.24.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (0.1.8)\n",
            "Requirement already satisfied: typing-extensions<4.6.0 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly) (4.5.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.10.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (2.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Er7DJXS3Ng3u"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import *\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "\n",
        "\n",
        "# INFERENCE GYM ISN'T WORKING RIGHT NOW DUE TO TF UPDATES... TRY LATER\n",
        "#!pip install _U inference_gym tfds_nightly\n",
        "#import inference_gym.using_jax as gym\n",
        "from inference_gym import using_jax as gym\n",
        "\n",
        "#from tensorflow_probability.spinoffs import using_jax as fun_mcmc\n",
        "#from fun_mc import using_jax as fun_mcmc\n",
        "\n",
        "#from tensorflow_probability.python.internal import prefer_static as ps\n",
        "#from tensorflow_probability.python.internal import unnest\n",
        "\n",
        "import tensorflow_probability as _tfp\n",
        "tfp = _tfp.substrates.jax\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "tfp_np = _tfp.substrates.numpy\n",
        "tfd_np = tfp_np.distributions\n",
        "\n",
        "#import arviz as az\n",
        "from tensorflow_probability.python.internal.unnest import get_innermost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with the 'Banana' Posterior Target Density\n",
        "\n",
        "target = gym.targets.VectorModel(gym.targets.Banana(),\n",
        "                                 flatten_sample_transformations = True)\n",
        "num_dimensions = target.event_shape[0]\n",
        "# Is the heuristic for the initial value of \\epsilon given in the paper used in practice?\n",
        "init_step_size = 1.\n",
        "\n",
        "#target = tfp.distributions.MultivariateNormalDiag()\n",
        "\n",
        "def target_log_prob_fn(x):\n",
        "   y = target.default_event_space_bijector(x)\n",
        "   fldj = target.default_event_space_bijector.forward_log_det_jacobian(x)\n",
        "   return target.unnormalized_log_prob(y) + fldj\n",
        "   #return x.log_prob\n",
        "\n",
        "offset = 2\n",
        "def bn_initialize(shape, key = random.PRNGKey(3727709)):\n",
        "  return 10 * random.normal(key, shape + (num_dimensions,)) + offset"
      ],
      "metadata": {
        "id": "J_4vsEveoV54"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running default HMC as written in Radford Neal's paper\n",
        "\n",
        "num_chains = 128\n",
        "num_super_chains = 4\n",
        "num_warmup, num_samples = 10, 10\n",
        "# 1000, 100\n",
        "total_samples = num_warmup + num_samples\n",
        "\n",
        "kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn, init_step_size, 1)\n",
        "kernel = tfp.experimental.mcmc.GradientBasedTrajectoryLengthAdaptation(kernel, num_warmup)\n",
        "kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(kernel, num_warmup,\n",
        "                                                  target_accept_prob = 0.75,\n",
        "                                                  reduce_fn = tfp.math.reduce_log_harmonic_mean_exp)\n",
        "\n",
        "# Initializing each chain (in a super chain) at the same location\n",
        "initial_state = bn_initialize((num_super_chains,))\n",
        "initial_state = jnp.repeat(initial_state, num_chains // num_super_chains, axis = 0)"
      ],
      "metadata": {
        "id": "FlX9TePtumEF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = tfp.mcmc.sample_chain(total_samples, initial_state,\n",
        "                               kernel = kernel, seed = random.PRNGKey(1954),\n",
        "                               trace_fn = None)"
      ],
      "metadata": {
        "id": "w5p4ybjEvuZs"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psbsofolgqvX",
        "outputId": "caa6a1d6-20a9-4e71-e85c-ce4103827364"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 128, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.executing_eagerly()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "nwoyTCcDv4nT",
        "outputId": "f33571b7-0615-4d90-8217-59d75c0ed3b7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-59fb998e27f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_conversion_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoc_typealias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_custom_casts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_float8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/core/_pywrap_custom_casts.so: undefined symbol: _ZN3tsl19RegisterCustomCastsEv, version tensorflow",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = jnp.arange(8)\n",
        "test = test.reshape((2, 2, 2))\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai8z6f4zgpSf",
        "outputId": "376b3c65-9292-4818-ab27-1b111ccc9e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0, 1],\n",
              "        [2, 3]],\n",
              "\n",
              "       [[4, 5],\n",
              "        [6, 7]]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.shape)\n",
        "result[:, :, 0].reshape(20, -1, 32, 1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dca2UDSejPhB",
        "outputId": "fb70123f-6d53-4485-8bd8-06134e804a3f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 128, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 4, 32, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#jnp.mean(test, axis = (0, 3))"
      ],
      "metadata": {
        "id": "STDX7D6jgwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define nested Rhat for one parameter.\n",
        "# This assumes the indexed parameter is a scalar, hence the result_state\n",
        "# tensor needs to be formatted accordingly.\n",
        "# TODO: deprecate state_is_list argument\n",
        "def nested_rhat_1dim(result_state, num_super_chains, index_param,\n",
        "                     num_samples, warmup_length = 0,\n",
        "                     rank_normalize = False):\n",
        "\n",
        "  state_param = result_state[warmup_length:(warmup_length + num_samples),\n",
        "                             :, index_param]\n",
        "\n",
        "  num_samples = state_param.shape[0]\n",
        "  num_chains = state_param.shape[1]\n",
        "  num_sub_chains = num_chains // num_super_chains\n",
        "  total_samples = num_samples * num_chains\n",
        "\n",
        "  if (rank_normalize):\n",
        "    state_param_flat = jnp.reshape(state_param, (total_samples, ))\n",
        "    temp = state_param_flat.argsort()\n",
        "    ranks = temp.argsort() + 1\n",
        "    z = norm.ppf((ranks - 3 / 8) / (total_samples + 1 / 4))\n",
        "    # This is misspelled in the original code\n",
        "    state_param = jnp.reshape(z, (num_samples, num_chains))\n",
        "\n",
        "  # The below operations work with the assumption that each of the superchains will have the same size\n",
        "  # Reshaping the array to account for that may not work in this case where M will change\n",
        "\n",
        "  # Reshaping to account for superchains\n",
        "  state_param = state_param.reshape(num_samples, -1, num_sub_chains, 1)\n",
        "\n",
        "  # Equation 14 in the paper\n",
        "  mean_chain = jnp.mean(state_param, axis = (0, 3))\n",
        "\n",
        "  # Piecewise equations in paper\n",
        "  between_chain_var = jnp.var(mean_chain, axis = 1, ddof = 1)\n",
        "  if (num_samples == 1):\n",
        "    mean_within_chain_var = 0\n",
        "  else:\n",
        "    within_chain_var = jnp.var(state_param, axis = (0, 3), ddof = 1)\n",
        "    mean_within_chain_var = jnp.mean(within_chain_var, axis = 1)\n",
        "\n",
        "  # Equation 17 in the paper\n",
        "  total_chain_var = between_chain_var + mean_within_chain_var\n",
        "\n",
        "  # Equation 16 in the paper\n",
        "  mean_super_chain = jnp.mean(state_param, axis = (0, 2, 3))\n",
        "  between_super_chain_var = jnp.var(mean_super_chain, ddof = 1)\n",
        "\n",
        "  # Equation 18 in the paper\n",
        "  return jnp.sqrt(1 + between_super_chain_var / jnp.mean(total_chain_var)),\\\n",
        "    between_super_chain_var, jnp.mean(total_chain_var)\n",
        "\n",
        "\n",
        "def nested_rhat(result_state, num_super_chains, index_param,\n",
        "                num_samples, warmup_length = 0, rank_normalize = False):\n",
        "  nRhat = jnp.array([])\n",
        "  B = jnp.array([])\n",
        "  W = jnp.array([])\n",
        "  for i in range(0, index_param.shape[0]):\n",
        "    nRhat_local, B_local, W_local = nested_rhat_1dim(result_state,\n",
        "                    num_super_chains, index_param[i], num_samples,\n",
        "                    warmup_length, rank_normalize)\n",
        "\n",
        "    nRhat = jnp.append(nRhat, nRhat_local)\n",
        "    B = jnp.append(B, B_local)\n",
        "    W = jnp.append(W, W_local)\n",
        "\n",
        "  return nRhat, B, W"
      ],
      "metadata": {
        "id": "UdjThIa-lxJT"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ChEES-HMC in order to get the most out of a GPU"
      ],
      "metadata": {
        "id": "Yog1nM3m2_Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select HMC Variant from options chees, snaper, or malt\n",
        "\n",
        "def construct_kernel(target_log_prob_fn, init_step_size, num_warmup, transition = 'chees'):\n",
        "  if transition == 'chees':\n",
        "    # Using gradient-based trajectory length adaptation and dual averaging defines ChEES HMC\n",
        "    kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn, init_step_size, 1)\n",
        "    kernel = tfp.experimental.mcmc.GradientBasedTrajectoryLengthAdaptation(kernel, num_warmup)\n",
        "    kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
        "        kernel, num_warmup, target_accept_prob = 0.75,\n",
        "        reduce_fn = tfp.math.reduce_log_harmonic_mean_exp)\n",
        "\n",
        "  elif transition == 'snaper':\n",
        "    # TO BE IMPLEMENTED\n",
        "    kernel = 'snaper'\n",
        "\n",
        "  elif transition == 'malt':\n",
        "    # TO BE IMPLEMENTED\n",
        "    kernel = 'malt'\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Not a valid transition kernel. Try one of ['chees', 'snaper', 'malt']\")\n",
        "\n",
        "  return kernel"
      ],
      "metadata": {
        "id": "l4mElY4y3Uyu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fits(num_seed, total_samples, initialize, kernel, num_super_chains,\n",
        "             index_param, num_samples, num_warmup, rank_normalize = False):\n",
        "\n",
        "  num_parameters = index_param.shape[0]\n",
        "\n",
        "  Rhat_list = jnp.zeros((num_seed, num_parameters))\n",
        "  nRhat_list = jnp.zeros((num_seed, num_parameters))\n",
        "  B_list = jnp.zeros((num_seed, num_parameters))\n",
        "  W_list = jnp.zeros((num_seed, num_parameters))\n",
        "  mc_mean_list = jnp.zeros((num_seed, num_parameters))\n",
        "\n",
        "  i = 0\n",
        "  for seed in jax.random.split(jax.random.PRNGKey(1), num_seed):\n",
        "    initial_state = initialize((num_super_chains,), key = seed + 1954)\n",
        "\n",
        "    initial_state = jnp.repeat(initial_state, num_chains // num_super_chains,\n",
        "                              axis = 0)\n",
        "\n",
        "    result = tfp.mcmc.sample_chain(total_samples, initial_state, kernel = kernel, seed = seed, trace_fn = None)\n",
        "\n",
        "    # Implementation of original R-Hat from Gelman and Rubin (1992)\n",
        "    Rhat_list = Rhat_list.at[i, :].set(tfp.mcmc.potential_scale_reduction(result[num_warmup:(num_warmup + num_samples), :, index_param]))\n",
        "\n",
        "    nRhat_local, B_local, W_local = nested_rhat(result,\n",
        "                                                num_super_chains = num_super_chains,\n",
        "                                                index_param = index_param,\n",
        "                                                num_samples = num_samples,\n",
        "                                                warmup_length = num_warmup,\n",
        "                                                rank_normalize = rank_normalize)\n",
        "    nRhat_list = nRhat_list.at[i, :].set(nRhat_local)\n",
        "    B_list = B_list.at[i, :].set(B_local)\n",
        "    W_list = W_list.at[i, :].set(W_local)\n",
        "\n",
        "    mc_mean_list = mc_mean_list.at[i, :].set(jnp.mean(result[num_warmup + 1, :, index_param], axis = 1))\n",
        "    i += 1\n",
        "\n",
        "  return Rhat_list, nRhat_list, B_list, W_list, mc_mean_list\n"
      ],
      "metadata": {
        "id": "yyDBbuoR3Zq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adaptive warmup scheme to produce MCMC samples\n",
        "def forge_chain(kernel_cold, kernel_warm, initial_state, num_super_chains,\n",
        "                num_warmup_array, num_samples, target_rhat, max_num_steps,\n",
        "                index_param, seed, num_nRhat_comp = 1, rank_normalize = False,\n",
        "                alpha_quantile = 1, mean_benchmark = None, var_benchmark = None):\n",
        "  warmup_is_acceptable = False\n",
        "  window_iteration = 0\n",
        "  current_state = initial_state\n",
        "  kernel_args = None\n",
        "\n",
        "  squared_err_list = jnp.array([])\n",
        "  nrhat_list = jnp.array([])\n",
        "\n",
        "  while(not warmup_is_acceptable and window_iteration < max_num_steps):\n",
        "\n",
        "    # Runs MCMC with a warmup window\n",
        "    result_cold, trace, kernel_args = tfp.mcmc.sample_chain(\n",
        "        num_results = num_warmup_array[window_iteration],\n",
        "        current_state = current_state,\n",
        "        kernel = kernel_cold,\n",
        "        previous_kernel_results = kernel_args,\n",
        "        trace_fn = lambda _, pkr: unnest.get_innermost(pkr, 'step_size'),\n",
        "        return_final_kernel_results = True,\n",
        "        seed = seed + window_iteration)\n",
        "\n",
        "    current_state = result_cold[-1]\n",
        "\n",
        "    # Generate candidate samples\n",
        "    result_warm, trace = tfp.mcmc.sample_chain(\n",
        "        num_results = num_samples*num_nRhat_comp,\n",
        "        current_state = current_state,\n",
        "        kernel = kernel_warm,\n",
        "        trace_fn = lambda _, pkr: unnest.get_innermost(pkr, 'step_size'),\n",
        "        previous_kernel_results = kernel_args,\n",
        "\n",
        "        # Why are we just adding numbers to seeds like this? I get it is to randomize but is there a more principled way?\n",
        "        seed = seed + 999999)\n",
        "\n",
        "    # Check if candidate samples are acceptable\n",
        "    nRhat = jnp.zeros((index_param.shape[0], num_nRhat_comp))\n",
        "    for i in range(0, num_nRhat_comp):\n",
        "      nR, _B, _W = nested_rhat(result_warm[i:((i+1)*num_samples)],\n",
        "                                        num_super_chains = num_super_chains,\n",
        "                                        index_param = index_param,\n",
        "                                        num_samples = num_samples,\n",
        "                                        rank_normalize = rank_normalize)\n",
        "      nRhat = nRhat.at[:, i].set(nR)\n",
        "\n",
        "    nRhat_quantile = jnp.quantile(jnp.mean(nRhat, axis = 1), alpha_quantile,\n",
        "                                 interpolation = \"nearest\")\n",
        "    print(\"nRhat_quantile: (\", alpha_quantile, \")\", nRhat_quantile)\n",
        "\n",
        "    if mean_benchmark is not None:\n",
        "      mc_mean = jnp.mean(result_warm[0, :, index_param], axis = 1)\n",
        "      squared_err = jnp.square(mc_mean - mean_benchmark[index_param])\\\n",
        "      / var_benchmark[index_param]\n",
        "\n",
        "      squared_err_list = jnp.append(squared_err_list, squared_err)\n",
        "      nrhat_list = jnp.append(nrhat_list, jnp.mean(nRhat, axis = 1))\n",
        "\n",
        "    if (nRhat_quantile < target_rhat): warmup_is_acceptable = True\n",
        "\n",
        "    window_iteration += 1\n",
        "\n",
        "  return result_warm, window_iteration, squared_err_list, nrhat_list"
      ],
      "metadata": {
        "id": "RggIF9Hqmxh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_forge_chain(num_seed, kernel_cold, kernel_warm, initialize, num_super_chains,\n",
        "                    num_warmup, num_samples, target_rhat, max_num_steps,\n",
        "                    index_param, num_nRhat_comp = 1, rank_normalize = False,\n",
        "                    alpha_quantile = 1, mean_benchmark = None,\n",
        "                    var_benchmark = None, initial_seed = 1):\n",
        "  mc_mean_list = jnp.zeros((num_seed, index_param.shape[0]))\n",
        "  warmup_length = jnp.zeros(num_seed)\n",
        "\n",
        "  squared_err_list_all = jnp.array([])\n",
        "  nrhat_list_all = jnp.array([])\n",
        "\n",
        "  i = 0\n",
        "  for seed in jax.random.split(jax.random.PRNGKey(initial_seed), num_seed):\n",
        "    print(\"NEW SEED\")\n",
        "    initial_state = initialize((num_super_chains,), key = seed + 1954)\n",
        "    initial_state = jnp.repeat(initial_state, num_chains // num_super_chains,\n",
        "                              axis = 0)\n",
        "\n",
        "    result, window_iteration, \\\n",
        "    squared_err_list, nrhat_list = forge_chain(kernel_cold, kernel_warm,\n",
        "                                               initial_state, num_super_chains,\n",
        "                                               num_warmup, num_samples,\n",
        "                                               target_rhat, max_num_steps,\n",
        "                                               index_param, seed,\n",
        "                                               num_nRhat_comp, rank_normalize,\n",
        "                                               alpha_quantile, mean_benchmark,\n",
        "                                               var_benchmark)\n",
        "\n",
        "    warmup_length = warmup_length.at[i].set(sum(num_warmup[:window_iteration]))\n",
        "    mc_mean_list = mc_mean_list.at[i, :].set(jnp.mean(result[0, :, index_param],\n",
        "                                 axis = 1))\n",
        "\n",
        "    squared_err_list_all = jnp.append(squared_err_list_all, squared_err_list)\n",
        "    nrhat_list_all = jnp.append(nrhat_list_all, nrhat_list)\n",
        "\n",
        "    i += 1\n",
        "  return mc_mean_list, warmup_length, squared_err_list_all, nrhat_list_all"
      ],
      "metadata": {
        "id": "48aMam844B1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num_warmup, num_samples = 1000, 100\n",
        "total_samples = num_warmup + num_samples + 1\n",
        "\n",
        "kernel = construct_kernel(target_log_prob_fn = target_log_prob_fn,\n",
        "                          init_step_size = init_step_size, num_warmup = num_warmup,\n",
        "                          transition = 'chees')\n",
        "index_param = jnp.array([0, 1])\n",
        "num_seed = 30\n",
        "rank_normalize = False"
      ],
      "metadata": {
        "id": "T25EaHnhZfmt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rhat_list, nRhat_list, B_list, W_list, mc_mean_list = run_fits(\n",
        "           num_seed = num_seed, total_samples = total_samples,\n",
        "           initialize = bn_initialize, kernel = kernel,\n",
        "           num_super_chains = K, index_param = index_param,\n",
        "           num_samples = num_samples, num_warmup = num_warmup,\n",
        "           rank_normalize = rank_normalize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DS_pAKjaNEa",
        "outputId": "c64e7ecd-154e-4df0-f1bb-d3b79a80c469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
            "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parellelizing the calculations of nRhat instead of sequential running\n",
        "# This involves getting rid of the loop inside of the code for the regular run_fits(...)\n",
        "# Does this change how the seeds are calculated at all?\n",
        "# Removing the num_seed argument if this won't be run sequentially\n",
        "\n",
        "# TURN THIS INTO A FUNCTION THAT IS VMAPPED\n",
        "def run_fits_parallel(iterations, total_samples, initialize, kernel, num_super_chains,\n",
        "             index_param, num_samples, num_warmup, rank_normalize = False):\n",
        "\n",
        "  num_parameters = index_param.shape[0]\n",
        "\n",
        "  Rhat_list = jnp.zeros((num_seed, num_parameters))\n",
        "  nRhat_list = jnp.zeros((num_seed, num_parameters))\n",
        "  B_list = jnp.zeros((num_seed, num_parameters))\n",
        "  W_list = jnp.zeros((num_seed, num_parameters))\n",
        "  mc_mean_list = jnp.zeros((num_seed, num_parameters))\n",
        "\n",
        "  # Should this be a more explicit parameter into the method for reproducibility purposes?\n",
        "  seed = jax.random.PRNGKey(1)\n",
        "  initial_state = initialize((num_super_chains*iterations,), key = seed + 1954)\n",
        "\n",
        "  initial_state = jnp.repeat(initial_state, num_chains*iterations // num_super_chains,\n",
        "                              axis = 0)\n",
        "\n",
        "  result = tfp.mcmc.sample_chain(total_samples, initial_state, kernel = kernel,\n",
        "                                 seed = seed, trace_fn = None)\n",
        "\n",
        "  # Loop over K superchains at a time in order to calculate nRhat statistics\n",
        "  # Investigate how JAX and looping may be impacting the speed of this loop\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # Selecting the subset that corresponds to K superchains of size M\n",
        "    current_result = result[:, (i*num_chains):((i+1)*num_chains), :]\n",
        "    print(current_result.shape)\n",
        "    # The Rhat_list argument probably should take some type of tensor object as an argument, and this is what is yielding an error right now\n",
        "    #Rhat_list = Rhat_list.at[i, :].set(tfp.mcmc.potential_scale_reduction)\n",
        "\n",
        "    # Implementation of original R-Hat from Gelman and Rubin (1992)\n",
        "\n",
        "    #Rhat_list = Rhat_list.at[i, :].set(tfp.mcmc.potential_scale_reduction(current_result))\n",
        "\n",
        "    nRhat_local, B_local, W_local = nested_rhat(current_result,\n",
        "                                                num_super_chains = num_super_chains,\n",
        "                                                index_param = index_param,\n",
        "                                                num_samples = num_samples,\n",
        "                                                warmup_length = num_warmup,\n",
        "                                                rank_normalize = rank_normalize)\n",
        "    nRhat_list = nRhat_list.at[i, :].set(nRhat_local)\n",
        "    B_list = B_list.at[i, :].set(B_local)\n",
        "    W_list = W_list.at[i, :].set(W_local)\n",
        "\n",
        "    mc_mean_list = mc_mean_list.at[i, :].set(jnp.mean(result[num_warmup + 1, (i*num_chains):((i+1)*num_chains), index_param], axis = 1))\n",
        "    i += 1\n",
        "\n",
        "  return Rhat_list, nRhat_list, B_list, W_list, mc_mean_list"
      ],
      "metadata": {
        "id": "lX3CPtM2lS0e"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0XfOdeaR0Hj",
        "outputId": "1a883ed5-83ef-4793-a618-fbaeac843b56"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIx came from slicing the result matrix too early\n",
        "run_fits_parallel(\n",
        "           iterations = num_seed, total_samples = total_samples,\n",
        "           initialize = bn_initialize, kernel = kernel,\n",
        "           num_super_chains = num_super_chains, index_param = index_param,\n",
        "           num_samples = num_samples, num_warmup = num_warmup,\n",
        "           rank_normalize = rank_normalize)"
      ],
      "metadata": {
        "id": "u1EsgGMNsWBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b33d5935-4ff2-4424-94c3-2fdae5cbc429"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n",
            "(20, 128, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]], dtype=float32),\n",
              " Array([[1.0285623, 1.0247593],\n",
              "        [1.0094608, 1.0032146],\n",
              "        [1.0180252, 1.0238867],\n",
              "        [1.0052211, 1.0078243],\n",
              "        [1.0055795, 1.008085 ],\n",
              "        [1.0061798, 1.0029197],\n",
              "        [1.00782  , 1.0021373],\n",
              "        [1.5657486, 1.447989 ],\n",
              "        [1.0055479, 1.0044115],\n",
              "        [1.0461769, 1.0343755],\n",
              "        [1.0008137, 1.0008119],\n",
              "        [1.0179238, 1.0077662],\n",
              "        [1.0293115, 1.0334105],\n",
              "        [1.0228231, 1.0201579],\n",
              "        [1.0086216, 1.0067111],\n",
              "        [1.0248461, 1.0082228],\n",
              "        [1.0060686, 1.0062075],\n",
              "        [1.0092676, 1.0105215],\n",
              "        [1.0086318, 1.0030895],\n",
              "        [1.0168439, 1.0057997],\n",
              "        [1.0013419, 1.0031674],\n",
              "        [1.0035803, 1.0047197],\n",
              "        [1.2588136, 1.0613686],\n",
              "        [1.0024343, 1.0029067],\n",
              "        [1.0028772, 1.0048635],\n",
              "        [1.0070322, 1.0004194],\n",
              "        [1.0229496, 1.0166727],\n",
              "        [1.0179317, 1.0091151],\n",
              "        [1.0018419, 1.003239 ],\n",
              "        [1.0083369, 1.0015477]], dtype=float32),\n",
              " Array([[5.15912056e-01, 1.21861950e-01],\n",
              "        [1.71067238e-01, 1.97646189e-02],\n",
              "        [4.33540642e-01, 1.55356660e-01],\n",
              "        [1.07838571e-01, 4.10461277e-02],\n",
              "        [1.02342509e-01, 3.88531499e-02],\n",
              "        [1.29918709e-01, 1.56217059e-02],\n",
              "        [1.48962677e-01, 1.17694475e-02],\n",
              "        [1.08147840e+01, 3.32825994e+00],\n",
              "        [7.86152184e-02, 3.64703275e-02],\n",
              "        [5.05843759e-01, 2.31623769e-01],\n",
              "        [1.06711565e-02, 6.67255744e-03],\n",
              "        [2.53070951e-01, 6.98459148e-02],\n",
              "        [4.70009655e-01, 3.51563871e-01],\n",
              "        [3.93948287e-01, 2.21984953e-01],\n",
              "        [1.42573833e-01, 7.32433945e-02],\n",
              "        [5.75147510e-01, 2.15728991e-02],\n",
              "        [1.32971272e-01, 1.44271692e-02],\n",
              "        [2.42043257e-01, 2.68401429e-02],\n",
              "        [1.89141780e-01, 7.75281666e-03],\n",
              "        [4.19634044e-01, 1.62857212e-02],\n",
              "        [2.65040658e-02, 8.03663768e-03],\n",
              "        [6.81272894e-02, 1.14655076e-02],\n",
              "        [5.30270386e+00, 1.65966064e-01],\n",
              "        [4.61807624e-02, 1.10461153e-02],\n",
              "        [5.52542210e-02, 1.74064413e-02],\n",
              "        [1.08541191e-01, 1.51697383e-03],\n",
              "        [4.21207279e-01, 6.95944279e-02],\n",
              "        [3.32318485e-01, 3.14390808e-02],\n",
              "        [3.03450823e-02, 1.27280038e-02],\n",
              "        [1.21117465e-01, 5.55503275e-03]], dtype=float32),\n",
              " Array([[ 8.904181 ,  2.4308286],\n",
              "        [ 8.9982395,  3.0692227],\n",
              "        [11.91857  ,  3.213565 ],\n",
              "        [10.300213 ,  2.6127586],\n",
              "        [ 9.145892 ,  2.393098 ],\n",
              "        [10.479169 ,  2.6712852],\n",
              "        [ 9.487305 ,  2.7503524],\n",
              "        [ 7.450411 ,  3.034873 ],\n",
              "        [ 7.0655246,  4.1245327],\n",
              "        [ 5.3536296,  3.3120828],\n",
              "        [ 6.553786 ,  4.1072726],\n",
              "        [ 6.996901 ,  4.4793386],\n",
              "        [ 7.901664 ,  5.1748114],\n",
              "        [ 8.533087 ,  5.4512024],\n",
              "        [ 8.232944 ,  5.43859  ],\n",
              "        [11.43214  ,  1.3063921],\n",
              "        [10.922581 ,  1.1584885],\n",
              "        [12.998367 ,  1.2688036],\n",
              "        [10.908939 ,  1.2527549],\n",
              "        [12.35248  ,  1.399967 ],\n",
              "        [ 9.868309 ,  1.2666173],\n",
              "        [ 9.49683  ,  1.2117689],\n",
              "        [ 9.070471 ,  1.3119512],\n",
              "        [ 9.473893 ,  1.8973134],\n",
              "        [ 9.587879 ,  1.7851655],\n",
              "        [ 7.690397 ,  1.8079424],\n",
              "        [ 9.072681 ,  2.069808 ],\n",
              "        [ 9.1838665,  1.7167374],\n",
              "        [ 8.22966  ,  1.9615847],\n",
              "        [ 7.2337623,  1.7931371]], dtype=float32),\n",
              " Array([[ 7.4007835 , -1.1151698 ],\n",
              "        [ 7.9368734 , -0.9561883 ],\n",
              "        [ 7.4494767 , -1.0574512 ],\n",
              "        [ 7.5932837 , -1.1401863 ],\n",
              "        [ 7.3353095 , -1.1261566 ],\n",
              "        [ 7.350517  , -1.2819097 ],\n",
              "        [ 7.9252853 , -0.7999593 ],\n",
              "        [ 9.955547  ,  0.42227858],\n",
              "        [13.075788  ,  2.2423341 ],\n",
              "        [12.890938  ,  2.1730695 ],\n",
              "        [13.015266  ,  2.2618532 ],\n",
              "        [12.919496  ,  2.2859397 ],\n",
              "        [13.280338  ,  2.4787822 ],\n",
              "        [12.921207  ,  2.0876224 ],\n",
              "        [12.89541   ,  2.2969685 ],\n",
              "        [ 1.1529673 , -2.6354065 ],\n",
              "        [ 1.0250206 , -2.8066802 ],\n",
              "        [ 0.9803442 , -2.7164955 ],\n",
              "        [ 1.2414188 , -2.7320979 ],\n",
              "        [ 0.87135565, -2.5498579 ],\n",
              "        [ 0.7944244 , -2.9388561 ],\n",
              "        [ 1.3073535 , -2.80442   ],\n",
              "        [ 3.3010876 , -2.298512  ],\n",
              "        [ 5.0585265 , -1.9697676 ],\n",
              "        [ 5.305467  , -1.8894415 ],\n",
              "        [ 5.410674  , -1.9106367 ],\n",
              "        [ 5.7279334 , -1.8445013 ],\n",
              "        [ 5.463623  , -1.8836787 ],\n",
              "        [ 5.463175  , -1.8285928 ],\n",
              "        [ 5.1224375 , -2.0656495 ]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def superchain_bootstrap(seed, result_state, num_chains, num_super_chains, index_param,\n",
        "                num_samples, warmup_length = 0, rank_normalize = False):\n",
        "\n",
        "  # Resampling from the available superchains to calculate a bootstrap sample\n",
        "  sc_selections = jax.random.choice(key = seed, a = num_super_chains,\n",
        "                                      shape = (num_super_chains,),\n",
        "                                            replace = True)\n",
        "\n",
        "  M = num_chains//num_super_chains\n",
        "\n",
        "  # Assigning first resampled superchain due to JAX array immutability\n",
        "  sc_bootstrapped = jax.lax.dynamic_slice_in_dim(result_state, sc_selections[0]*M, M, axis = 1)\n",
        "  for index in sc_selections[1:]:\n",
        "    # Creating a new sample using bootstrapped superchains, and computing Nested Rhat\n",
        "    sc_bootstrapped = jnp.concatenate((sc_bootstrapped, jax.lax.dynamic_slice_in_dim(result_state, index*M, M, axis = 1)), axis = 1)\n",
        "  return nested_rhat(sc_bootstrapped, num_super_chains, index_param, num_samples, warmup_length, rank_normalize)[0]\n",
        "\n",
        "superchain_bootstrap = jax.vmap(superchain_bootstrap, in_axes = (0, None, None, None, None, None), out_axes = 0)\n"
      ],
      "metadata": {
        "id": "SuUZrfkN_tG6"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = jax.random.PRNGKey(17)\n",
        "keys = jax.random.split(seed, 3)\n",
        "superchain_bootstrap(keys, result, num_chains, num_super_chains, index_param, num_samples)"
      ],
      "metadata": {
        "id": "IK331mLGDMS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5514a24-0bfd-45ef-cb26-acbf45920cc3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[3.1204305, 2.7081726],\n",
              "       [3.7372012, 3.1370656],\n",
              "       [4.5191708, 4.0229588]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3IrYMAmghG3",
        "outputId": "d87d21dd-cbf8-45eb-9269-d850f4e2edea"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[-1.3588713e+01,  5.7870488e+00],\n",
              "        [-1.3062107e+01,  4.8002548e+00],\n",
              "        [-1.4049887e+01,  4.9101820e+00],\n",
              "        ...,\n",
              "        [-1.3397634e-02, -3.7039132e+00],\n",
              "        [ 3.2161176e-02, -3.4378133e+00],\n",
              "        [ 2.0687485e-01, -5.6175876e+00]],\n",
              "\n",
              "       [[-1.3588713e+01,  5.7870488e+00],\n",
              "        [-1.3062107e+01,  4.8002548e+00],\n",
              "        [-1.4049887e+01,  4.9101820e+00],\n",
              "        ...,\n",
              "        [-1.3397634e-02, -3.7039132e+00],\n",
              "        [ 3.2161176e-02, -3.4378133e+00],\n",
              "        [ 2.0687485e-01, -5.6175876e+00]],\n",
              "\n",
              "       [[-1.3588713e+01,  5.7870488e+00],\n",
              "        [-1.3062107e+01,  4.8002548e+00],\n",
              "        [-1.4049887e+01,  4.9101820e+00],\n",
              "        ...,\n",
              "        [-1.3397634e-02, -3.7039132e+00],\n",
              "        [-1.7051940e+00, -2.2336993e+00],\n",
              "        [ 2.0687485e-01, -5.6175876e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.3778205e+01,  3.6130025e+00],\n",
              "        [-1.1565995e+01, -3.1337008e-01],\n",
              "        [-1.7491156e+01,  6.9300065e+00],\n",
              "        ...,\n",
              "        [-5.1843560e-01, -2.2952733e+00],\n",
              "        [-9.8615539e-01, -5.3210559e+00],\n",
              "        [ 2.2783992e+00, -3.5192456e+00]],\n",
              "\n",
              "       [[-1.4068560e+01,  2.1354301e+00],\n",
              "        [-1.0964580e+01,  7.3828769e-01],\n",
              "        [-1.8536743e+01,  6.7848649e+00],\n",
              "        ...,\n",
              "        [-1.4881599e-01, -3.9542654e+00],\n",
              "        [-1.1490175e+00, -3.8592393e+00],\n",
              "        [ 2.6416163e+00, -3.8677788e+00]],\n",
              "\n",
              "       [[-1.2912323e+01,  3.2895913e+00],\n",
              "        [-1.0413697e+01,  1.0547349e+00],\n",
              "        [-1.8328741e+01,  6.7143621e+00],\n",
              "        ...,\n",
              "        [-1.0962288e+00, -3.4684088e+00],\n",
              "        [-5.3754193e-01, -2.9805703e+00],\n",
              "        [ 3.1180198e+00, -4.4188962e+00]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.shape)\n",
        "jnp.take(result, jnp.array(jnp.where(groups == 0)), axis = 1).shape"
      ],
      "metadata": {
        "id": "mViw22d-Vyic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8584fb-d39e-4599-9273-01940162c643"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 128, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1, 38, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.array([1, 2, 3, 4, 1])\n",
        "jnp.unique(x, return_counts = True)"
      ],
      "metadata": {
        "id": "p3kuahkwX5aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ff79ae-4cc0-413a-ec65-f65ecca723e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([1, 2, 3, 4], dtype=int32), Array([2, 1, 1, 1], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Rhat needs to be rewritten for the possibility where we resample individual chains\n",
        "def nested_rhat_1dim_bootstrap(result_state, super_chain_groups, index_param,\n",
        "                     num_samples, warmup_length = 0,\n",
        "                     rank_normalize = False):\n",
        "\n",
        "  # Selecting only samples after burn-in\n",
        "  initial_state_param = result_state[warmup_length:(warmup_length + num_samples),\n",
        "                             :, index_param]\n",
        "\n",
        "  # Setting variables for size of stored markov chains\n",
        "  num_samples = initial_state_param.shape[0]\n",
        "  num_chains = initial_state_param.shape[1]\n",
        "  super_chains, chain_lengths = jnp.unique(super_chain_groups, return_counts = True)\n",
        "  total_samples = num_samples * num_chains\n",
        "\n",
        "\n",
        "  # Reshaping array to format new superchains of varying length\n",
        "  state_param = jnp.take(initial_state_param, jnp.array(jnp.where(super_chain_groups == 0)), axis = 1)\n",
        "  for sc in super_chains[1:]:\n",
        "    new_super_chain = jnp.take(initial_state_param, jnp.array(jnp.where(super_chain_groups == sc)), axis = 1)\n",
        "    state_param = jnp.concatenate((state_param, new_super_chain), axis = 2)\n",
        "\n",
        "  # \\tilde{B}_k in the paper, which goes into equation 17\n",
        "  between_chain_var = jnp.zeros(len(super_chains))\n",
        "  start = 0\n",
        "  for super in range(len(super_chains)):\n",
        "    #print(state_param[:, :, start:(start+chain_lengths[super])].shape)\n",
        "    subchain_means = jnp.mean(state_param[:, :, start:(start+chain_lengths[super])], axis = (0,1))\n",
        "    between_chain_var = between_chain_var.at[super].set(jnp.var(subchain_means, ddof = 1))\n",
        "    start += chain_lengths[super]\n",
        "\n",
        "  # \\tilde{W}_k in the paper, which goes into equation 17\n",
        "  if (num_samples == 1):\n",
        "    mean_within_chain_var = 0\n",
        "  else:\n",
        "    mean_within_chain_var = jnp.zeros(len(super_chains))\n",
        "    start = 0\n",
        "    for super in range(len(super_chains)):\n",
        "      within_chain_var = jnp.var(state_param[:, :, start:(start + chain_lengths[super])], axis = (0,1), ddof = 1)\n",
        "      mean_within_chain_var = mean_within_chain_var.at[super].set(jnp.mean(within_chain_var))\n",
        "      start += chain_lengths[super]\n",
        "\n",
        "  # Equation 17 in the paper\n",
        "  # This part should remain the same\n",
        "  total_chain_var = between_chain_var + mean_within_chain_var\n",
        "\n",
        "  # Equation 14 in the paper\n",
        "  mean_super_chain = jnp.zeros(len(super_chains))\n",
        "  start = 0\n",
        "  for super in range(len(super_chains)):\n",
        "    # This is for creating the mean chain\n",
        "    # An axis shouldn't need to be specified here, since we are working within each superchain\n",
        "    mean_super_chain = mean_super_chain.at[super].set(jnp.mean(state_param[:, :,start:(start+chain_lengths[super])]))\n",
        "    start += chain_lengths[super]\n",
        "\n",
        "  # Equation 16 in the paper\n",
        "  between_super_chain_var = jnp.var(mean_super_chain, ddof = 1)\n",
        "\n",
        "  # Equation 18 in the paper\n",
        "  # This equation should be the same\n",
        "  return jnp.sqrt(1 + between_super_chain_var / jnp.mean(total_chain_var)),\\\n",
        "    between_super_chain_var, jnp.mean(total_chain_var)\n",
        "  #return between_chain_var, mean_within_chain_var\n",
        "\n",
        "\n",
        "def nested_rhat_bootstrap(result_state, super_chain_groups, index_param,\n",
        "                num_samples, warmup_length = 0, rank_normalize = False):\n",
        "  nRhat = jnp.array([])\n",
        "  B = jnp.array([])\n",
        "  W = jnp.array([])\n",
        "  for i in range(0, index_param.shape[0]):\n",
        "    nRhat_local, B_local, W_local = nested_rhat_1dim_bootstrap(result_state,\n",
        "                    super_chain_groups, index_param[i], num_samples,\n",
        "                    warmup_length, rank_normalize)\n",
        "\n",
        "    nRhat = jnp.append(nRhat, nRhat_local)\n",
        "    B = jnp.append(B, B_local)\n",
        "    W = jnp.append(W, W_local)\n",
        "\n",
        "  return nRhat, B, W"
      ],
      "metadata": {
        "id": "2jx6uWD7qtKZ"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of superchain indices for each markov chain\n",
        "g = jnp.concatenate((jnp.repeat(0, 32), jnp.repeat(1, 32), jnp.repeat(2, 32), jnp.repeat(3, 32)))"
      ],
      "metadata": {
        "id": "_4oXHpffuF0j"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nested_rhat(result, num_super_chains, index_param, num_samples, warmup_length = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDg5iDvDtag3",
        "outputId": "d3aa95d5-68c4-4116-b5e3-a72ffd821f12"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([3.120422 , 2.9483204], dtype=float32),\n",
              " Array([77.25679 , 34.424973], dtype=float32),\n",
              " Array([8.842452, 4.475081], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The between chain variance is the exact same, so the error lies in the within chain variances\n",
        "nested_rhat_bootstrap(result, g, index_param, num_samples, warmup_length = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnOSTn6Rt6K1",
        "outputId": "22629f90-fcb5-4e97-8558-fbaeec47995b"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([3.120422 , 2.9483206], dtype=float32),\n",
              " Array([77.25679, 34.42498], dtype=float32),\n",
              " Array([8.842452, 4.475081], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.mean(result[:, 0, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOQw3g1Q4lrz",
        "outputId": "2e2e6d1c-e200-4458-b099-3c59dcafad5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.15447582, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a second bootstrapped variance estimator\n",
        "# This should be \"naive\" as well, where each Markov Chain is bootstrapped\n",
        "# It is important to keep track of which superchain each chain belongs to\n",
        "# The number of chains within each superchain will not necessarily be the same size anymore\n",
        "\n",
        "def individual_bootstrap(seed, result_state, num_chains, num_super_chains, index_param,\n",
        "                num_samples, warmup_length = 0, rank_normalize = False):\n",
        "\n",
        "  M = num_chains // num_super_chains\n",
        "\n",
        "  # Selecting indices of bootstrap to compute\n",
        "  bootstraps = jax.random.choice(key = seed, a = num_chains,\n",
        "                                      shape = (num_chains,),\n",
        "                                            replace = True)\n",
        "\n",
        "  # Tracking which superchain each sample is associated with\n",
        "  superchains = bootstraps // M\n",
        "  bootstrapped_chains = jnp.reshape(result_state[:, bootstraps[0], :], (result_state.shape[0], -1, result_state.shape[2]))\n",
        "  for chain in bootstraps[1:]:\n",
        "    new_sample = jnp.reshape(result_state[:, chain, :], (result_state.shape[0], -1, result_state.shape[2]))\n",
        "    bootstrapped_chains = jnp.concatenate((bootstrapped_chains, new_sample), axis = 1)\n",
        "  return nested_rhat_bootstrap(bootstrapped_chains, bootstraps, superchains, index_param, num_samples, warmup_length), bootstrapped_chains, bootstraps, superchains"
      ],
      "metadata": {
        "id": "LSMUCFUtSWk0"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_super_chains"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9OwKuyg1-8U",
        "outputId": "6216038f-a9c8-423a-eb7c-823977819689"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_param = jnp.array([0,1])\n",
        "nr, bs, indices, chains = individual_bootstrap(seed, result, 128, 4, index_param, 10, warmup_length = 10)"
      ],
      "metadata": {
        "id": "EySyNd90vHWP"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chains"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhleBP6R4c0s",
        "outputId": "2b1ac834-2759-4020-baca-b0d95f52e2f6"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([3, 0, 2, 2, 2, 3, 3, 1, 0, 2, 2, 2, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3,\n",
              "       0, 0, 3, 3, 1, 1, 2, 2, 0, 1, 0, 1, 1, 1, 0, 2, 2, 1, 3, 1, 3, 1,\n",
              "       1, 3, 1, 0, 3, 1, 0, 3, 3, 2, 0, 0, 1, 1, 1, 0, 1, 2, 2, 3, 0, 2,\n",
              "       2, 2, 1, 2, 1, 0, 1, 3, 2, 0, 0, 2, 0, 0, 3, 1, 2, 1, 0, 3, 0, 2,\n",
              "       1, 1, 0, 0, 3, 1, 0, 3, 3, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       2, 2, 0, 3, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nested_rhat_bootstrap(bs, chains, index_param, num_samples, warmup_length = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1rl0Z5yyQKY",
        "outputId": "0d438ede-d7bb-4c67-8a29-25551bf5dfb6"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([3.4333248, 3.1380036], dtype=float32),\n",
              " Array([80.44966 , 37.081467], dtype=float32),\n",
              " Array([7.4575214, 4.1913857], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_initialize(target_log_prob_fn, init_step_size, num_warmup, warmup_window):\n",
        "    kernel_cold = construct_kernel(target_log_prob_fn = target_log_prob_fn,\n",
        "                                   init_step_size = init_step_size,\n",
        "                                   num_warmup = num_warmup)\n",
        "    kernel_warm = construct_kernel(target_log_prob_fn = target_log_prob_fn,\n",
        "                                   init_step_size = init_step_size,\n",
        "                                   num_warmup = num_warmup)\n",
        "    window_array = jnp.append(jnp.repeat(10, 10),\n",
        "                              jnp.repeat(warmup_window,\n",
        "                                         num_warmup // warmup_window - 1))\n",
        "    nRhat_upper = 1.05\n",
        "\n",
        "    try:\n",
        "      mean_est = target.sample_transformations['identity'].ground_truth_mean\n",
        "    except:\n",
        "      print('no ground truth mean')\n",
        "      mean_est = (result[num_warmup:, :]).mean(0).mean(0)\n",
        "    try:\n",
        "      var_est = target.sample_transformations['identity'].ground_truth_standard_deviation**2\n",
        "    except:\n",
        "      print('no ground truth std dev')\n",
        "      var_est = ((result[num_warmup:, :]**2).mean(0).mean(0) -\n",
        "                mean_est**2)\n",
        "\n",
        "    return kernel_cold, kernel_warm, window_array, nRhat_upper, mean_est, var_est"
      ],
      "metadata": {
        "id": "mOOZ-jXjfjYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will look into doing different runs across different GPUs to compute what nRhat should be\n",
        "# Also think about ESS and what it would look like in this scenario"
      ],
      "metadata": {
        "id": "jK39_3OTTdqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We will now focus on variance measurement of nested Rhat, with users having the\n",
        "possibility to choose several parameters such as the number of superchains (K),\n",
        "number of markov chains in each group of superchains (M), and number of draws in\n",
        "the sampling phase (N). We will measure this in both a brute force manner, as\n",
        "well as coming up with a notion of bootstrapping. We also can decide to run this\n",
        "variance estimate across all chains sequentially, or in parallel. Having an\n",
        "adaptive warmup length, or a fixed numbers of warmup iterations is also a\n",
        "potential parameter\n",
        "\n",
        "Using an initial seed value of 1\n",
        "\n",
        "I need to think a bit about parallelization as well as bootstrapping\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# There's 8 possibilities here that come from 2^[boostrap, adaptive, parallel]\n",
        "def variance_estimate(target_log_prob_fn, index_param, num_seed = 10, num_warmup = 1000, num_samples = 5,\n",
        "                      num_super_chains = 4, initialize = bn_initialize, num_chains = 128,\n",
        "                      bootstrap = False, adaptive = False, parallel = False,\n",
        "                      warmup_window = 100, initial_seed = 1):\n",
        "\n",
        "  if adaptive:\n",
        "\n",
        "    kernel_cold, kernel_warm, window_array, \\\n",
        "    nRhat_upper, mean_est, var_est = adaptive_initialize(target_log_prob_fn,\n",
        "                                                         init_step_size,\n",
        "                                                         num_warmup,\n",
        "                                                         warmup_window)\n",
        "\n",
        "    if bootstrap:\n",
        "      if parallel:\n",
        "        # Adpative, Bootstrapped, and Parallel\n",
        "\n",
        "        return\n",
        "\n",
        "      else:\n",
        "        # Adaptive, Bootstrapped, and Sequential\n",
        "        return\n",
        "\n",
        "    else:\n",
        "      if parallel:\n",
        "        # Adpative, Brute-Force, and Parallel\n",
        "        return\n",
        "\n",
        "      else:\n",
        "        # Adaptive, Brute-Force, and Sequential\n",
        "        mc_mean_list, warmup_length, squared_error_list, \\\n",
        "        nRhat_list = run_forge_chain(num_seed = num_seed,\n",
        "                                     kernel_cold = kernel_cold,\n",
        "                                     kernel_warm = kernel_warm,\n",
        "                                     initialize = initialize,\n",
        "                                     num_super_chains = num_super_chains,\n",
        "                                     num_warmup = window_array,\n",
        "                                     num_samples = num_samples,\n",
        "                                     target_rhat = nRhat_upper,\n",
        "                                     max_num_steps = window_array.shape[0],\n",
        "                                     index_param = index_param,\n",
        "                                     rank_normalize = False,\n",
        "                                     alpha_quantile = 1.,\n",
        "                                     mean_benchmark = mean_est,\n",
        "                                     var_benchmark = var_est)\n",
        "\n",
        "  else:\n",
        "\n",
        "    kernel = construct_kernel(target_log_prob_fn = target_log_prob_fn,\n",
        "                              init_step_size = init_step_size,\n",
        "                              num_warmup = num_warmup)\n",
        "    if bootstrap:\n",
        "\n",
        "      if parallel:\n",
        "        # Fixed, Bootstrapped, and Parallel\n",
        "        return\n",
        "\n",
        "      else:\n",
        "        # Fixed, Bootstrapped, and Sequential\n",
        "        return\n",
        "\n",
        "    else:\n",
        "      if parallel:\n",
        "        # Fixed, Brute-Force, and Parallel\n",
        "        return\n",
        "\n",
        "      else:\n",
        "        # Fixed, Brute-Force, and Sequential\n",
        "        # Adress apparent discrepancy in definition of total_samples\n",
        "        Rhat_list, nRhat_list, B_list, W_list, mc_mean_list = run_fits(\n",
        "            num_seed = num_seed, total_samples = num_warmup + num_samples + 1,\n",
        "            initialize = initialize, kernel = kernel,\n",
        "            num_super_chains = num_super_chains,\n",
        "            index_param = index_param, num_samples = num_samples,\n",
        "            num_warmup = num_warmup)\n",
        "\n",
        "  return nRhat_list"
      ],
      "metadata": {
        "id": "33hWczNl4Tmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}